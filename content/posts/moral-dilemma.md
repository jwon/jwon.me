---
title: Moral Dilemma
date: 2012-06-18T21:17:13+00:00
aliases: [/2012/06/18/moral-dilemma/]
toc:
  enable: false
tags:
  - Opinion
  - Technology

---
Given a situation were we have self-driving cars, what is the "right" decision?

The self-driving car is in a situation where it is on a crash course to hit 5 people (likely killing all of them) or can do a hard turn and crash into a brick wall instead (likely killing the driver).

What decision should it programmatically make?

Since the driver purchased the self-driving car, should the car prioritize the life of the driver?
